üéì Systematic Learning Plan: Master Regression Analysis

  ---
  üìö Phase 1: Foundation (Day 1-2, ~3-4 hours)

  Goal: Understand what each line DOES, not just what it says

  Step 1: The Data Journey

  Start here - Section 1 & 2 (Data Loading):

  1. Read the cell, then close the notebook
  2. On paper, answer:
    - What is this cell trying to do?
    - What would happen if I removed line X?
    - Why do we need this step?
  3. Make small modifications (learn by breaking):
  # Original
  df = pd.read_csv('data/student_salary_data.csv')

  # TRY THIS: What happens?
  df = pd.read_csv('data/student_salary_data.csv', nrows=50)
  print(f"Loaded only {len(df)} rows")

  # TRY THIS: What columns do we have?
  print("Column names:", df.columns.tolist())
  print("Data types:", df.dtypes)

  # TRY THIS: What's the salary range?
  print(f"Min salary: ${df['Salary'].min():,.0f}")
  print(f"Max salary: ${df['Salary'].max():,.0f}")
  print(f"Average: ${df['Salary'].mean():,.0f}")
  4. Questions to answer:
    - Why do we check for missing values BEFORE modeling?
    - What happens if we DON'T impute missing values?
    - Why mean imputation? Why not median or mode?

  Hands-On Experiment 1: Break It to Learn It

  Try this modification in Section 2 (Missing Data):

  # EXPERIMENT: What if we DON'T impute?
  df_no_imputation = df.copy()
  # Skip imputation - keep NaNs

  # Try to calculate correlation - what happens?
  try:
      correlation = df_no_imputation.corr()
      print("Correlation worked!")
  except Exception as e:
      print(f"ERROR: {e}")
      print("LESSON: NaNs break calculations!")

  # NOW impute and see the difference
  df_imputed = df.fillna(df.mean(numeric_only=True))
  correlation = df_imputed.corr()
  print("After imputation, correlation works!")

  What you learn: Why data cleaning matters before analysis.

  ---
  üîç Phase 2: Visualization Deep-Dive (Day 2-3, ~2-3 hours)

  Goal: Understand what plots MEAN, not just how to make them

  Section 3: Univariate Analysis

  Don't just run the histogram code - EXPERIMENT:

  # Original histogram
  plt.hist(df['Salary'], bins=30)

  # EXPERIMENT 1: Change bins
  fig, axes = plt.subplots(1, 3, figsize=(15, 4))

  axes[0].hist(df['Salary'], bins=5, alpha=0.7, edgecolor='black')
  axes[0].set_title('5 bins - TOO FEW (loses detail)')

  axes[1].hist(df['Salary'], bins=30, alpha=0.7, edgecolor='black')
  axes[1].set_title('30 bins - JUST RIGHT')

  axes[2].hist(df['Salary'], bins=100, alpha=0.7, edgecolor='black')
  axes[2].set_title('100 bins - TOO MANY (noisy)')

  plt.tight_layout()
  plt.show()

  print("LESSON: Bin size affects what patterns you see!")

  Questions to answer:
  - What does the shape of the histogram tell you? (Normal? Skewed? Bimodal?)
  - Why do we show mean AND median? What's the difference?
  - What does a boxplot outlier actually mean? (Q1 - 1.5*IQR rule)

  Hands-On Experiment 2: Outliers

  # EXPERIMENT: What happens if we REMOVE outliers?
  def remove_outliers(data, column):
      Q1 = data[column].quantile(0.25)
      Q3 = data[column].quantile(0.75)
      IQR = Q3 - Q1
      lower = Q1 - 1.5 * IQR
      upper = Q3 + 1.5 * IQR
      return data[(data[column] >= lower) & (data[column] <= upper)]

  df_no_outliers = remove_outliers(df, 'Salary')

  print(f"Original data: {len(df)} rows")
  print(f"After removing outliers: {len(df_no_outliers)} rows")
  print(f"Lost {len(df) - len(df_no_outliers)} rows
  ({(len(df)-len(df_no_outliers))/len(df)*100:.1f}%)")

  print(f"\nSalary statistics:")
  print(f"With outliers - Mean: ${df['Salary'].mean():,.0f}, Std:
  ${df['Salary'].std():,.0f}")
  print(f"Without outliers - Mean: ${df_no_outliers['Salary'].mean():,.0f}, Std:
   ${df_no_outliers['Salary'].std():,.0f}")

  print("\nLESSON: Outliers affect mean and std deviation!")

  ---
  üìä Phase 3: Correlation & Relationships (Day 3-4, ~2-3 hours)

  Goal: Understand correlation ‚â† causation, and what correlations MEAN

  Section 4: Bivariate Analysis

  Deep-dive into correlation:

  # Look at the correlation matrix
  correlation_matrix = df.corr()

  # EXPERIMENT: What's the difference?
  print("="*60)
  print("UNDERSTANDING CORRELATION")
  print("="*60)

  # Pair 1: GPA vs Salary
  gpa_salary_corr = df[['GPA', 'Salary']].corr().iloc[0, 1]
  print(f"\n1. GPA vs Salary: r = {gpa_salary_corr:.3f}")
  print(f"   Interpretation: {'Strong' if abs(gpa_salary_corr) > 0.7 else
  'Moderate' if abs(gpa_salary_corr) > 0.4 else 'Weak'} {'positive' if
  gpa_salary_corr > 0 else 'negative'} relationship")
  print(f"   Meaning: As GPA increases, salary tends to {'increase' if
  gpa_salary_corr > 0 else 'decrease'}")

  # Pair 2: Internships vs Salary
  intern_salary_corr = df[['Internships_Count', 'Salary']].corr().iloc[0, 1]
  print(f"\n2. Internships vs Salary: r = {intern_salary_corr:.3f}")
  print(f"   Interpretation: {'Stronger' if abs(intern_salary_corr) >
  abs(gpa_salary_corr) else 'Weaker'} than GPA")

  # CRITICAL LESSON
  print("\n" + "="*60)
  print("‚ö†Ô∏è   CORRELATION ‚â† CAUSATION")
  print("="*60)
  print("High correlation means:")
  print("  ‚úì Variables move together")
  print("  ‚úì One can PREDICT the other")
  print("\nHigh correlation does NOT mean:")
  print("  ‚úó One CAUSES the other")
  print("  ‚úó There's no third variable involved")
  print("\nExample: Ice cream sales correlate with drowning deaths")
  print("  ‚Üí NOT because ice cream causes drowning!")
  print("  ‚Üí BOTH caused by summer/hot weather (confounding variable)")

  Hands-On Experiment 3: Scatter Plots

  # EXPERIMENT: Visualize correlation strength
  fig, axes = plt.subplots(1, 3, figsize=(16, 4))

  # Strong correlation
  axes[0].scatter(df['GPA'], df['Salary'], alpha=0.6)
  axes[0].set_title(f'Strong Correlation (r={gpa_salary_corr:.2f})\nClear linear
   pattern')
  axes[0].set_xlabel('GPA')
  axes[0].set_ylabel('Salary')

  # Weak correlation
  age_salary_corr = df[['Age', 'Salary']].corr().iloc[0, 1]
  axes[1].scatter(df['Age'], df['Salary'], alpha=0.6)
  axes[1].set_title(f'Weak Correlation (r={age_salary_corr:.2f})\nScattered
  points')
  axes[1].set_xlabel('Age')
  axes[1].set_ylabel('Salary')

  # No correlation (create random data)
  random_data = np.random.randn(len(df))
  axes[2].scatter(random_data, df['Salary'], alpha=0.6)
  axes[2].set_title('No Correlation (r‚âà0)\nRandom cloud')
  axes[2].set_xlabel('Random Variable')
  axes[2].set_ylabel('Salary')

  plt.tight_layout()
  plt.show()

  print("LESSON: Correlation strength = how tight the linear pattern is")

  ---
  üéØ Phase 4: Simple Regression MASTERY (Day 4-5, ~3-4 hours)

  Goal: Truly understand what regression IS and what coefficients MEAN

  Section 6: Simple Linear Regression

  This is THE most important section to master!

  Step-by-Step Deep Dive:

  # The regression equation: Salary = Œ≤‚ÇÄ + Œ≤‚ÇÅ * GPA + Œµ

  # STEP 1: Fit the model
  X = sm.add_constant(df['GPA'])
  y = df['Salary']
  model = sm.OLS(y, X).fit()

  # STEP 2: Extract coefficients
  beta_0 = model.params['const']  # Intercept
  beta_1 = model.params['GPA']    # Slope

  print("="*60)
  print("UNDERSTANDING REGRESSION COEFFICIENTS")
  print("="*60)

  print(f"\nŒ≤‚ÇÄ (Intercept) = ${beta_0:,.2f}")
  print(f"  ‚Üí Predicted salary when GPA = 0")
  print(f"  ‚Üí Not meaningful here (no one has GPA = 0)")
  print(f"  ‚Üí Think of it as the 'baseline' salary")

  print(f"\nŒ≤‚ÇÅ (Slope) = ${beta_1:,.2f}")
  print(f"  ‚Üí For every 1-point increase in GPA...")
  print(f"  ‚Üí ...salary increases by ${beta_1:,.2f}")
  print(f"\n  CONCRETE EXAMPLE:")
  print(f"    Student A: GPA = 2.5 ‚Üí Predicted salary = ${beta_0 +
  beta_1*2.5:,.2f}")
  print(f"    Student B: GPA = 3.5 ‚Üí Predicted salary = ${beta_0 +
  beta_1*3.5:,.2f}")
  print(f"    Difference: ${beta_1:,.2f} (exactly Œ≤‚ÇÅ!)")

  Hands-On Experiment 4: Build Your Own Predictions

  # EXPERIMENT: Predict salaries for different GPAs
  print("\n" + "="*60)
  print("MAKING PREDICTIONS")
  print("="*60)

  test_gpas = [2.0, 2.5, 3.0, 3.5, 4.0]

  for gpa in test_gpas:
      predicted_salary = beta_0 + beta_1 * gpa
      print(f"GPA {gpa:.1f} ‚Üí Predicted Salary: ${predicted_salary:,.0f}")

  # Compare to actual data
  print(f"\nActual average salaries:")
  for gpa_bin in [2.0, 2.5, 3.0, 3.5]:
      actual = df[(df['GPA'] >= gpa_bin) & (df['GPA'] < gpa_bin +
  0.5)]['Salary'].mean()
      predicted = beta_0 + beta_1 * (gpa_bin + 0.25)
      print(f"GPA {gpa_bin:.1f}-{gpa_bin+0.5:.1f}: Actual=${actual:,.0f},
  Predicted=${predicted:,.0f}")

  print("\nLESSON: Predictions are close but not perfect (that's the error term
  Œµ)")

  Understanding R-squared:

  r_squared = model.rsquared

  print("\n" + "="*60)
  print("UNDERSTANDING R-SQUARED")
  print("="*60)

  print(f"\nR¬≤ = {r_squared:.4f} = {r_squared*100:.2f}%")

  print(f"\nWhat this means:")
  print(f"  ‚Üí {r_squared*100:.1f}% of salary variation is explained by GPA")
  print(f"  ‚Üí {(1-r_squared)*100:.1f}% is explained by OTHER factors")

  print(f"\nVisualize it:")
  print(f"  Imagine 100 students with different salaries")
  print(f"  ‚Üí {r_squared*100:.0f} students: salary differences due to GPA")
  print(f"  ‚Üí {(1-r_squared)*100:.0f} students: salary differences due to OTHER
  things")
  print(f"      (internships, major, luck, location, etc.)")

  print(f"\nR¬≤ interpretation guide:")
  print(f"  0.0-0.3: Weak model (most variation unexplained)")
  print(f"  0.3-0.7: Moderate model (some predictive power)")
  print(f"  0.7-1.0: Strong model (most variation explained)")
  print(f"\n  Your model: {'Weak' if r_squared < 0.3 else 'Moderate' if
  r_squared < 0.7 else 'Strong'}")

  Understanding P-values & Hypothesis Testing:

  p_value = model.pvalues['GPA']
  t_stat = model.tvalues['GPA']

  print("\n" + "="*60)
  print("UNDERSTANDING P-VALUES & SIGNIFICANCE")
  print("="*60)

  print(f"\nHypothesis Test:")
  print(f"  H‚ÇÄ (Null): Œ≤‚ÇÅ = 0 (GPA has NO effect on salary)")
  print(f"  H‚Çê (Alternative): Œ≤‚ÇÅ ‚â† 0 (GPA DOES affect salary)")

  print(f"\nResults:")
  print(f"  t-statistic = {t_stat:.4f}")
  print(f"    ‚Üí How many standard errors Œ≤‚ÇÅ is from 0")
  print(f"    ‚Üí |t| > 2 typically means significant")

  print(f"\n  p-value = {p_value:.6f}")
  print(f"    ‚Üí Probability of seeing this result if H‚ÇÄ were true")
  print(f"    ‚Üí Think: 'How surprising is this result?'")

  if p_value < 0.001:
      print(f"\n  DECISION: REJECT H‚ÇÄ (p < 0.001)")
      print(f"    ‚Üí VERY strong evidence that GPA affects salary")
      print(f"    ‚Üí Less than 0.1% chance this is random")
  elif p_value < 0.05:
      print(f"\n  DECISION: REJECT H‚ÇÄ (p < 0.05)")
      print(f"    ‚Üí Significant evidence that GPA affects salary")
      print(f"    ‚Üí Less than 5% chance this is random")
  else:
      print(f"\n  DECISION: FAIL TO REJECT H‚ÇÄ (p > 0.05)")
      print(f"    ‚Üí Insufficient evidence")

  print("\n‚ö†Ô∏è   IMPORTANT:")
  print("  'Reject H‚ÇÄ' does NOT mean 'GPA causes salary'")
  print("  It means: 'GPA and salary are associated'")
  print("  Causation requires more evidence (experiments, theory, etc.)")

  ---
  üöÄ Phase 5: Multiple Regression (Day 5-6, ~3-4 hours)

  Goal: Understand "holding other variables constant"

  Section 7: Multiple Regression

  The KEY insight:

  # Simple regression
  simple_model = sm.OLS(df['Salary'], sm.add_constant(df['GPA'])).fit()
  simple_gpa_coef = simple_model.params['GPA']

  # Multiple regression (with internships, projects, etc.)
  X_multi = sm.add_constant(df[['GPA', 'Internships_Count',
  'Projects_Completed']])
  multi_model = sm.OLS(df['Salary'], X_multi).fit()
  multi_gpa_coef = multi_model.params['GPA']

  print("="*60)
  print("SIMPLE VS MULTIPLE REGRESSION")
  print("="*60)

  print(f"\nGPA coefficient:")
  print(f"  Simple regression: ${simple_gpa_coef:,.2f}")
  print(f"  Multiple regression: ${multi_gpa_coef:,.2f}")
  print(f"  Difference: ${multi_gpa_coef - simple_gpa_coef:,.2f}")

  print(f"\nWhy the difference?")
  print(f"  Simple regression: GPA effect includes:")
  print(f"    ‚Üí Direct effect of GPA")
  print(f"    ‚Üí Indirect effects (high GPA ‚Üí more internships ‚Üí higher salary)")
  print(f"\n  Multiple regression: GPA effect is ONLY:")
  print(f"    ‚Üí Direct effect of GPA")
  print(f"    ‚Üí 'Holding internships constant'")

  print(f"\nAnalogy:")
  print(f"  Simple: 'Students with higher GPA earn ${simple_gpa_coef:,.0f}
  more'")
  print(f"  Multiple: 'Students with higher GPA earn ${multi_gpa_coef:,.0f}
  more,")
  print(f"            EVEN IF they have the same internships/projects'")

  Understanding Dummy Variables:

  print("\n" + "="*60)
  print("UNDERSTANDING DUMMY VARIABLES")
  print("="*60)

  # Example: Major (Computer Science, Data Science, Statistics, Business
  Analytics)
  # We drop Business Analytics (reference category)

  print("\nDummy encoding example:")
  print(f"  Student 1: Major = 'Computer Science'")
  print(f"    ‚Üí Major_CS = 1, Major_DS = 0, Major_Stats = 0")
  print(f"\n  Student 2: Major = 'Data Science'")
  print(f"    ‚Üí Major_CS = 0, Major_DS = 1, Major_Stats = 0")
  print(f"\n  Student 3: Major = 'Business Analytics' (REFERENCE)")
  print(f"    ‚Üí Major_CS = 0, Major_DS = 0, Major_Stats = 0")

  # Get coefficients (after running multiple regression)
  cs_coef = multi_model.params.get('Major_Computer Science', 0)

  print(f"\n\nInterpreting coefficient:")
  print(f"  Major_CS coefficient = ${cs_coef:,.2f}")
  print(f"\n  Meaning: CS majors earn ${cs_coef:,.2f} MORE than")
  print(f"           Business Analytics majors (reference),")
  print(f"           holding GPA, internships, etc. CONSTANT")

  print(f"\n  Example:")
  print(f"    Student A: Business Analytics, GPA 3.0, 2 internships")
  print(f"      ‚Üí Predicted salary = [baseline]")
  print(f"    Student B: Computer Science, GPA 3.0, 2 internships")
  print(f"      ‚Üí Predicted salary = [baseline] + ${cs_coef:,.0f}")
  print(f"\n    The ONLY difference is major!")

  ---
  üî¨ Phase 6: Diagnostics & Advanced (Day 6-7, ~2-3 hours)

  Goal: Understand WHY assumptions matter

  Hands-On Experiment 5: What If Assumptions Fail?

  # EXPERIMENT: Create data that VIOLATES assumptions

  # 1. Non-linearity
  print("="*60)
  print("EXPERIMENT 1: NON-LINEARITY")
  print("="*60)

  # Create quadratic relationship
  x = np.linspace(0, 10, 100)
  y_true = 2*x**2 + 5  # True relationship is QUADRATIC
  y_observed = y_true + np.random.normal(0, 10, 100)  # Add noise

  # Fit LINEAR model (WRONG!)
  X_wrong = sm.add_constant(x)
  model_wrong = sm.OLS(y_observed, X_wrong).fit()

  # Plot
  fig, axes = plt.subplots(1, 2, figsize=(14, 5))

  # Left: Scatterplot
  axes[0].scatter(x, y_observed, alpha=0.6, label='Data')
  axes[0].plot(x, model_wrong.predict(X_wrong), 'r-', linewidth=2, label='Linear
   fit (WRONG)')
  axes[0].plot(x, y_true, 'g--', linewidth=2, label='True relationship
  (quadratic)')
  axes[0].set_title('Data with Non-linear Relationship')
  axes[0].legend()

  # Right: Residuals (should show pattern)
  residuals = y_observed - model_wrong.predict(X_wrong)
  axes[1].scatter(model_wrong.predict(X_wrong), residuals, alpha=0.6)
  axes[1].axhline(0, color='red', linestyle='--')
  axes[1].set_title('Residuals (shows PATTERN = bad!)')
  axes[1].set_xlabel('Fitted values')
  axes[1].set_ylabel('Residuals')

  plt.tight_layout()
  plt.show()

  print("LESSON: If relationship is non-linear, residuals show a pattern")
  print("FIX: Add x¬≤ term, or transform variables")

  ---
  üìù Phase 7: Create Your Own Study Guide (Day 7, ~2 hours)

  Document Your Learning

  Create a new notebook cell at the END with this:

  # üéì My Understanding Checklist

  ## Simple Regression
  - [ ] I can explain what Œ≤‚ÇÄ and Œ≤‚ÇÅ mean in plain English
  - [ ] I can interpret R¬≤ without looking at notes
  - [ ] I understand why p < 0.05 matters
  - [ ] I can explain correlation ‚â† causation with an example
  - [ ] I can make predictions by hand (without code)

  ## Multiple Regression
  - [ ] I can explain "holding other variables constant"
  - [ ] I understand why coefficients change from simple to multiple
  - [ ] I can interpret dummy variable coefficients
  - [ ] I know when to use multiple vs simple regression

  ## Diagnostics
  - [ ] I can explain each assumption (linearity, normality, etc.)
  - [ ] I know what residual plots should look like
  - [ ] I understand what VIF measures
  - [ ] I can explain what to do if assumptions fail

  ## Communication
  - [ ] I can present these results to a non-technical audience
  - [ ] I can answer "So what?" for each finding
  - [ ] I can defend my modeling choices

  ---
  üéØ Active Learning Exercises

  Exercise 1: Teach It Back

  Record yourself (or write out) explaining:
  1. "What is regression?" (30 seconds)
  2. "What does this coefficient mean?" (1 minute)
  3. "Why do we check assumptions?" (1 minute)

  If you can't explain it simply, you don't understand it yet.

  Exercise 2: Change One Thing

  For EACH section:
  1. Change ONE number/parameter
  2. Predict what will happen
  3. Run it and see if you're right
  4. If wrong, figure out WHY

  Example:
  # Original
  bins = 30

  # Prediction: "If I use 5 bins, I'll see less detail in the histogram"
  # Try it:
  bins = 5
  # Result: Was I right?

  Exercise 3: Break It on Purpose

  # Try to make the regression FAIL
  # What happens if you:
  1. Remove all observations with GPA > 3.0?
  2. Add 100 duplicates of one student?
  3. Make all salaries the same?
  4. Use a categorical variable without dummies?

  # Each failure teaches you something!

  ---
  üìö Study Resources

  For Visual Learners:

  - StatQuest videos on YouTube (Josh Starmer) - AMAZING explanations
  - 3Blue1Brown - Linear Algebra intuition

  For Reading:

  - "An Introduction to Statistical Learning" (free PDF online)
  - Your notebook's markdown cells (I wrote detailed explanations!)

  For Practice:

  - Kaggle datasets - try this analysis on different data
  - Make up your own data with numpy - control everything!

  ---
  ‚úÖ Final Mastery Test

  You've mastered this when you can:

  1. ‚úÖ Explain the entire analysis to a friend who doesn't know stats
  2. ‚úÖ Modify any coefficient and predict the impact
  3. ‚úÖ Identify which assumptions are violated just by looking at plots
  4. ‚úÖ Defend your modeling choices in your presentation
  5. ‚úÖ Apply this to a DIFFERENT dataset from scratch

  ---
  üöÄ Your Homework Presentation Prep

  Week before presentation:

  Day 1-2: Deep understanding (this plan)
  Day 3-4: Practice explaining out loud
  Day 5: Create presentation slides (5-10 slides max)
  Day 6: Dry run with a friend/roommate
  Day 7: Final polish

  Slide structure:
  1. Problem (1 slide)
  2. Data (1 slide)
  3. Key findings (2-3 slides)
  4. Diagnostics (1 slide)
  5. Recommendations (1 slide)

